{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNC6wzAYhNlqtuofCoi1j8x",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madelinelai/Kaggle/blob/main/titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPbvstVxpUd9"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = \"https://github.com/Madelinelai/Kaggle/raw/main/titanic/train.csv\"\n",
        "urlretrieve(url, \"train.csv\")\n",
        "url = \"https://github.com/Madelinelai/Kaggle/raw/main/titanic/test.csv\"\n",
        "urlretrieve(url, \"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUlGE2xJrcIS"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\", encoding=\"utf-8\")\n",
        "test_df = pd.read_csv(\"test.csv\", encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4aP1_JfriAv"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4WS1tUVrkXo"
      },
      "source": [
        "#資料預處理－缺失值填補\n",
        "#step1　欄位分類　Cabin(船頭)，Embarked\n",
        "#step2  PClass.Name(中間名), Sex\n",
        "#step3  Age,SibSp,Parch,Ticket(算出同行有多少人), Fare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IumybOphuXBn"
      },
      "source": [
        "data = pd.concat([train_df, test_df], ignore_index=True)\n",
        "data = data.drop([\"PassengerId\", \"Survived\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGfjl_C5wX6p"
      },
      "source": [
        "# 此行是檢查資料裡是不是有N/A\n",
        "na = data.isna().sum()\n",
        "# Series[帶入根你的資料筆數一樣多True/False list]\n",
        "na[na > 0].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBKY9MtxX6P-"
      },
      "source": [
        "#補上最常出現的類別/補上中位數\n",
        "#Cabin有空值，use apply\n",
        "#觀念說明如下\n",
        "#a = pd.Series([1,2,3])\n",
        "#def func(n):\n",
        "#  return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeXGIM6uUxTS"
      },
      "source": [
        "#如果不是空值　就回傳 s / 是空值就會回傳N/A or None\n",
        "def cabin_head(s):\n",
        "    if not pd.isna(s):\n",
        "        return s[0]\n",
        "data[\"Cabin\"] = data[\"Cabin\"].apply(cabin_head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir-in0TvWSFx"
      },
      "source": [
        "#算出同行有多少人，換句話說有多少人一起分享同張票\n",
        "dic = data[\"Ticket\"].value_counts()\n",
        "data[\"Ticket\"] = data[\"Ticket\"].apply(lambda t:dic[t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxRKqq_1WqQx"
      },
      "source": [
        "#補缺失值，補最常出現（類別Embarked：最常出現缺失值\u001d\n",
        "#如果一堆測試資料，不要重算，直接補s\n",
        "most = data['Embarked'].value_counts().idxmax()\n",
        "data['Embarked'] = data['Embarked'].fillna(most)\n",
        "na = data.isna().sum()\n",
        "#Series(帶入，根據你的資料筆數一樣多True/False list)\n",
        "na[na>0].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyGtFW9fwXyr"
      },
      "source": [
        "# 補缺失值(數值: 中位數)\n",
        "med = data.median().drop([\"Pclass\"])\n",
        "data = data.fillna(med)\n",
        "na = data.isna().sum()\n",
        "# Series[帶入根你的資料筆數一樣多True/False list]\n",
        "na[na > 0].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7vN5dbewXo8"
      },
      "source": [
        "#錄製影片有詳細解說\n",
        "\n",
        "def name_convert(s):\n",
        "    s = s.split(\",\")[-1].split(\".\")[0]\n",
        "    s = s.strip()\n",
        "    return s\n",
        "counts = data[\"Name\"].apply(name_convert).value_counts()\n",
        "whitelist = counts[counts > 50].index\n",
        "def name_convert(s):\n",
        "    s = s.split(\",\")[-1].split(\".\")[0]\n",
        "    s = s.strip()\n",
        "    if s in whitelist:\n",
        "        return s\n",
        "    else:\n",
        "        return None\n",
        "data[\"Name\"] = data[\"Name\"].apply(name_convert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv956eRZwXbk"
      },
      "source": [
        "#再次檢視資料中其它欄位是否有需要ETL\n",
        "#PClass屬大小類別123,Sex屬二值型，略過不必做\n",
        "#整理完之後為２５個欄位，如下：\n",
        "data = pd.get_dummies(data)\n",
        "data = pd.get_dummies(data, columns=[\"Pclass\"])\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMEWPakBzutj"
      },
      "source": [
        "#新增一個family欄位，故合併 SibSp+Parch（兄弟姐妹父母）\n",
        "#以上是預處理的欄位嘗試能提昇Model正確率,千萬不要任意的刪除資料中的欄位\n",
        "data[\"Family\"] = data[\"SibSp\"] + data[\"Parch\"]\n",
        "data\n",
        "#整理完之後為２６個欄位，如下："
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QH1MWL77Yl"
      },
      "source": [
        "print(len(data))\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94L5Os1G0fg3"
      },
      "source": [
        "# .loc (根據列編號)(X) .iloc (根據第幾個)(O)\n",
        "# .iloc [第一列, 第二列, 第三列...]\n",
        "x_train = data.iloc[:train_df.shape[0]]\n",
        "y_train = train_df[\"Survived\"]\n",
        "x_predict = data.iloc[train_df.shape[0]:]\n",
        "# x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "647bb5FN0fd7"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKliBzw80fXo"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "params = {\n",
        "    # 1. 5 2. [1, 2, 3] 3. range\n",
        "    # 20~99\n",
        "    \"n_estimators\":range(20, 100),\n",
        "    # 3~10\n",
        "    \"max_depth\":range(3, 11)\n",
        "}\n",
        "clf = RandomForestClassifier()\n",
        "cv = GridSearchCV(clf, params, cv=10, n_jobs=-1)\n",
        "cv.fit(x_train, y_train)\n",
        "print(cv.best_score_)\n",
        "print(cv.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BayhSWY10tkv"
      },
      "source": [
        "#can look Scikit-learn/selection.cross_val_score\n",
        "clf = RandomForestClassifier(n_estimators=25, max_depth=6)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=10, n_jobs=-1)\n",
        "print(\"10:\", scores)\n",
        "print(\"average:\", np.average(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jX5rh02XztkJ"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=97, max_depth=8)\n",
        "clf.fit(x_train, y_train)\n",
        "pre = clf.predict(x_predict)\n",
        "df = pd.DataFrame({\n",
        "    \"PassengerId\":test_df[\"PassengerId\"],\n",
        "    \"Survived\":pre\n",
        "})\n",
        "df.to_csv(\"rf.csv\", encoding=\"utf-8\", index=False)\n",
        "#up kaggle I submission scored 0.77751\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBajCrvFxIYx"
      },
      "source": [
        "# Use Tree look feauture_importances based on RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CE59COCxK9y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "print(len(clf.estimators_))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plot_tree(clf.estimators_[2], \n",
        "          feature_names=data.columns, \n",
        "          class_names=[\"Dead\", \"Alived\"],\n",
        "          max_depth=2,\n",
        "          filled=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnO08yKdxH29"
      },
      "source": [
        "pd.DataFrame({\n",
        "    \"Name\":data.columns,\n",
        "    \"Importance\":clf.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "# sum(clf.feature_importances_) ＃將所有特徵分數，加總合會是１"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "icwtTEjyz3tE"
      },
      "source": [
        "#觀察性別與生存之比例\n",
        "import seaborn as sns\n",
        "sns.countplot(x=train_df[\"Sex\"], hue=train_df[\"Survived\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1kJcaS_z3f-"
      },
      "source": [
        "#觀察票價種類與生存之比例\n",
        "plt.figure(figsize=(10, 10))\n",
        "c = pd.cut(train_df[\"Fare\"], bins=10)\n",
        "sns.countplot(c, hue=train_df[\"Survived\"])\n",
        "plt.xticks(rotation=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u3ozGAT9z3SO"
      },
      "source": [
        "#觀察年紀與生存之比例\n",
        "plt.figure(figsize=(10, 10))\n",
        "c = pd.cut(train_df[\"Age\"], bins=10)\n",
        "sns.countplot(c, hue=train_df[\"Survived\"])\n",
        "plt.xticks(rotation=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UgXivQOM0lH7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzmkoN4e0k00"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ncG_RHU50kwV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIgfYu01LFgr"
      },
      "source": [
        "#嘗試使用KNN. (需要做Scaler), 將中心點定住"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jm9on0F0fFA"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "data_scale = scaler.fit_transform(data)\n",
        "#再一次將Data轉為DataFrame\n",
        "data_scale = pd.DataFrame(data_scale, columns=data.columns)\n",
        "# .loc (根據列編號)(X) .iloc (根據第幾個)(O)\n",
        "# .iloc [第一列, 第二列, 第三列...]\n",
        "x_train_scale = data_scale.iloc[:train_df.shape[0]]\n",
        "x_predict_scale = data_scale.iloc[train_df.shape[0]:]\n",
        "x_train_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69gCpS3gLY7D"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "params = {\n",
        "    \"n_neighbors\":range(3, 100)\n",
        "}\n",
        "clf = KNeighborsClassifier()\n",
        "cv = GridSearchCV(clf, params, cv=10, n_jobs=-1)\n",
        "cv.fit(x_train_scale, y_train)\n",
        "print(cv.best_score_)\n",
        "print(cv.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgJJe_ztL7lX"
      },
      "source": [
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=11)\n",
        "clf.fit(x_train_scale, y_train)\n",
        "pre = clf.predict(x_predict_scale)\n",
        "df = pd.DataFrame({\n",
        "    \"PassengerId\":test_df[\"PassengerId\"],\n",
        "    \"Survived\":pre\n",
        "})\n",
        "df.to_csv(\"knn.csv\", encoding=\"utf-8\", index=False)\n",
        "df\n",
        "#In kaggle I submission scored 0.80143"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFZVcOpIupPI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD8vURFYupA7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7oc44OmioHI"
      },
      "source": [
        "# 嘗試使用GradientBoostingClassifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PGj8OzFMXAa"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "combined = pd.concat([train.drop('Survived',axis=1),test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTE42ViYk1AR"
      },
      "source": [
        "combined.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNXG5p4Klvt7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "train['Age'].fillna(train['Age'].median(),inplace=True) # Imputing Missing Age Values\n",
        "train['Embarked'].fillna(train['Embarked'].value_counts().index[0], inplace=True) # Imputing Missing Embarked Values\n",
        "d = {1:'1st',2:'2nd',3:'3rd'} #Creating a dictionary to convert Passenger Class from 1,2,3 to 1st,2nd,3rd.\n",
        "train['Pclass'] = train['Pclass'].map(d) #Mapping the column based on the dictionary\n",
        "train.drop(['PassengerId','Name','Ticket','Cabin'], 1, inplace=True) # Dropping Unnecessary Columns\n",
        "categorical_vars = train[['Pclass','Sex','Embarked']] # Getting Dummies of Categorical Variables\n",
        "dummies = pd.get_dummies(categorical_vars,drop_first=True)\n",
        "train = train.drop(['Pclass','Sex','Embarked'],axis=1) #Dropping the Original Categorical Variables to avoid duplicates\n",
        "train = pd.concat([train,dummies],axis=1) #Now, concat the new dummy variables\n",
        "train.head() #Check the clean version of the train data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyjzX9wGmEeN"
      },
      "source": [
        "# Splitting Features and Label\n",
        "y = train['Survived']\n",
        "X = train.drop(['Survived'],1)\n",
        "\n",
        "#Using Train Test Split from Sklearn to Split Our Train Dataset into Train and Testing Datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LhdD58PmOuC"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(learning_rate=0.1,max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCDKvUHtmTbZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "#I get about 80% accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmxhNx02mXwJ"
      },
      "source": [
        "test['Age'].fillna(test['Age'].median(),inplace=True) # Age\n",
        "test['Fare'].fillna(test['Fare'].median(),inplace=True) # Fare\n",
        "d = {1:'1st',2:'2nd',3:'3rd'} #Pclass\n",
        "test['Pclass'] = test['Pclass'].map(d)\n",
        "test['Embarked'].fillna(test['Embarked'].value_counts().index[0], inplace=True) # Embarked\n",
        "ids = test[['PassengerId']]# Passenger Ids\n",
        "test.drop(['PassengerId','Name','Ticket','Cabin'],1,inplace=True)# Drop Unnecessary Columns\n",
        "categorical_vars = test[['Pclass','Sex','Embarked']]# Get Dummies of Categorical Variables\n",
        "dummies = pd.get_dummies(categorical_vars,drop_first=True)\n",
        "test = test.drop(['Pclass','Sex','Embarked'],axis=1)#Drop the Original Categorical Variables\n",
        "test = pd.concat([test,dummies],axis=1)#Instead, concat the new dummy variables\n",
        "#test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdgL4iyrmrLj"
      },
      "source": [
        "preds = model.predict(test)\n",
        "results = ids.assign(Survived=preds)\n",
        "results.to_csv('titanic_submission.csv',index=False)\n",
        "#In kaggle I submission scored 0.73684"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkJMhR-F41BN"
      },
      "source": [
        "#嘗試使用ANN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K0cR0kAK40vW"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "#%matplotlib inline # keep this only if you are using iPython\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "ids = test[['PassengerId']] # I will use Passenger Ids in the final csv file.\n",
        "combined = pd.concat([train, test], axis=0, sort=False)\n",
        "#Age Column\n",
        "combined['Age'].fillna(combined['Age'].median(),inplace=True) # Age\n",
        "combined['Embarked'].fillna(combined['Embarked'].value_counts().index[0], inplace=True) # Embarked\n",
        "combined['Fare'].fillna(combined['Fare'].median(),inplace=True)\n",
        "d = {1:'1st',2:'2nd',3:'3rd'} #Pclass\n",
        "combined['Pclass'] = combined['Pclass'].map(d) #Pclass\n",
        "combined['Child'] = train['Age'].apply(lambda age: 1 if age>=18 else 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMYDUIJt40hO"
      },
      "source": [
        "combined.iloc[:10]['Name']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h9ekJa-g40H5"
      },
      "source": [
        "# Example Input --> 'Futrelle, Mrs. Jacques Heath (Lily May Peel)'\n",
        "combined['Title'] = combined['Name'].str.split('.').str.get(0)  # output : 'Futrelle, Mrs'\n",
        "combined['Title'] = combined['Title'].str.split(',').str.get(1) # output : 'Mrs '\n",
        "combined['Title'] = combined['Title'].str.strip()               # output : 'Mrs'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yA20PYTz6Bbo"
      },
      "source": [
        "combined.groupby('Title').count()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5bcVVbBi6E7l"
      },
      "source": [
        "french_titles = ['Don', 'Dona', 'Mme', 'Ms', 'Mra','Mlle']\n",
        "english_titles = ['Mr', 'Mrs','Mrs','Mrs','Mrs','Miss']\n",
        "for i in range(len(french_titles)):\n",
        "    for j in range(len(english_titles)):\n",
        "        if i == j:\n",
        "            combined['Title'] = combined['Title'].str.replace(french_titles[i],english_titles[j])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzLW7wj26J_I"
      },
      "source": [
        "major_titles = ['Mr','Mrs','Miss','Master'] # And we will also have others for the other titles.\n",
        "combined['Title'] = combined['Title'].apply(lambda title: title if title in major_titles else 'Others')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1bHcyNBi6N7U"
      },
      "source": [
        "#Dropping the Irrelevant Columns\n",
        "combined.drop(['PassengerId','Name','Ticket','Cabin'], 1, inplace=True)\n",
        "\n",
        "# Getting Dummy Variables and Dropping the Original Categorical Variables\n",
        "categorical_vars = combined[['Pclass','Sex','Embarked','Title','Child']] # Get Dummies of Categorical Variables\n",
        "dummies = pd.get_dummies(categorical_vars,drop_first=True)\n",
        "combined = combined.drop(['Pclass','Sex','Embarked','Title','Child'],axis=1)\n",
        "combined = pd.concat([combined, dummies],axis=1)\n",
        "\n",
        "# Separating Train and Test Datasets\n",
        "test = combined[combined['Survived'].isnull()].drop(['Survived'],axis=1)\n",
        "train = combined[combined['Survived'].notnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qVedkEHW6RZi"
      },
      "source": [
        "y = train['Survived']\n",
        "X = train.drop(['Survived'],1)\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "sc = StandardScaler()\n",
        "X = sc.fit_transform(X)\n",
        "test = sc.fit_transform(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bq5mFsEK6VMG"
      },
      "source": [
        "#Building the ANN Model\n",
        "from keras.wrappers.scikit_learn import KerasClassifier\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout\n",
        "def build_classifier(optimizer):\n",
        "    classifier = Sequential()\n",
        "    classifier.add(Dense(units=10,kernel_initializer='uniform',activation='relu',input_dim=14))\n",
        "    classifier.add(Dropout(rate = 0.2))\n",
        "    classifier.add(Dense(units=128,kernel_initializer='uniform',activation='relu'))\n",
        "    classifier.add(Dropout(rate = 0.2))\n",
        "    classifier.add(Dense(units=1,kernel_initializer='uniform',activation='sigmoid'))\n",
        "    classifier.compile(optimizer=optimizer,loss='binary_crossentropy',metrics=['accuracy'])\n",
        "    return classifier"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXiS6xiL6cSw"
      },
      "source": [
        "#Grid Search Cross-Validation\n",
        "#use scikit-learn GridSearchCV to find the best parameters and tune our ANN to get the best results. \n",
        "#Try different optimizers, epochs, and batch_sizes with the following code\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "classifier = KerasClassifier(build_fn = build_classifier)\n",
        "param_grid = dict(optimizer = ['Adam'],\n",
        "                  epochs=[10, 20, 50],\n",
        "                  batch_size=[16, 25, 32])\n",
        "grid = GridSearchCV(estimator=classifier, param_grid=param_grid, scoring='accuracy')\n",
        "grid_result = grid.fit(X, y)\n",
        "best_parameters = grid.best_params_\n",
        "best_accuracy = grid.best_score_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4dK_7fL6k-I"
      },
      "source": [
        "#Fitting the Model with Best Parameters\n",
        "classifier = KerasClassifier(build_fn = build_classifier,\n",
        "                             optimizer=best_parameters['optimizer'],\n",
        "                             batch_size=best_parameters['batch_size'],\n",
        "                             epochs=best_parameters['epochs'])\n",
        "\n",
        "classifier.fit(X,y)\n",
        "preds = classifier.predict(test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqS_q8Eq7U86"
      },
      "source": [
        "results = ids.assign(Survived=preds)\n",
        "results['Survived'] = results['Survived'].apply(lambda row: 1 if row > 0.5 else 0)\n",
        "results.to_csv('ANNlai.csv',index=False)\n",
        "#up Kaggle I submission scored 0.78708\n",
        "results.head(20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6uMUBBl7oUa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}