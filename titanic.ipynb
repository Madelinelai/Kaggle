{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "titanic.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNJRhH0wb3dc9rN/CQ0EklW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Madelinelai/Kaggle/blob/main/titanic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BPbvstVxpUd9"
      },
      "source": [
        "from urllib.request import urlretrieve\n",
        "url = \"https://github.com/Madelinelai/Kaggle/raw/main/titanic/train.csv\"\n",
        "urlretrieve(url, \"train.csv\")\n",
        "url = \"https://github.com/Madelinelai/Kaggle/raw/main/titanic/test.csv\"\n",
        "urlretrieve(url, \"test.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IUlGE2xJrcIS"
      },
      "source": [
        "import pandas as pd\n",
        "train_df = pd.read_csv(\"train.csv\", encoding=\"utf-8\")\n",
        "test_df = pd.read_csv(\"test.csv\", encoding=\"utf-8\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4aP1_JfriAv"
      },
      "source": [
        "test_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4WS1tUVrkXo"
      },
      "source": [
        "#資料預處理－缺失值填補\n",
        "#step1　欄位分類　Cabin(船頭)，Embarked\n",
        "#step2  PClass.Name(中間名), Sex\n",
        "#step3  Age,SibSp,Parch,Ticket(算出同行有多少人), Fare"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IumybOphuXBn"
      },
      "source": [
        "data = pd.concat([train_df, test_df], ignore_index=True)\n",
        "data = data.drop([\"PassengerId\", \"Survived\"], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WGfjl_C5wX6p"
      },
      "source": [
        "# 此行是檢查資料裡是不是有N/A\n",
        "na = data.isna().sum()\n",
        "# Series[帶入根你的資料筆數一樣多True/False list]\n",
        "na[na > 0].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eBKY9MtxX6P-"
      },
      "source": [
        "#補上最常出現的類別/補上中位數\n",
        "#Cabin有空值，use apply\n",
        "#觀念說明如下\n",
        "#a = pd.Series([1,2,3])\n",
        "#def func(n):\n",
        "#  return n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZeXGIM6uUxTS"
      },
      "source": [
        "#如果不是空值　就回傳 s / 是空值就會回傳N/A or None\n",
        "def cabin_head(s):\n",
        "    if not pd.isna(s):\n",
        "        return s[0]\n",
        "data[\"Cabin\"] = data[\"Cabin\"].apply(cabin_head)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ir-in0TvWSFx"
      },
      "source": [
        "#算出同行有多少人，換句話說有多少人一起分享同張票\n",
        "dic = data[\"Ticket\"].value_counts()\n",
        "data[\"Ticket\"] = data[\"Ticket\"].apply(lambda t:dic[t])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hxRKqq_1WqQx"
      },
      "source": [
        "#補缺失值，補最常出現（類別Embarked：最常出現缺失值\u001d\n",
        "#如果一堆測試資料，不要重算，直接補s\n",
        "most = data['Embarked'].value_counts().idxmax()\n",
        "data['Embarked'] = data['Embarked'].fillna(most)\n",
        "na = data.isna().sum()\n",
        "#Series(帶入，根據你的資料筆數一樣多True/False list)\n",
        "na[na>0].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wyGtFW9fwXyr"
      },
      "source": [
        "# 補缺失值(數值: 中位數)\n",
        "med = data.median().drop([\"Pclass\"])\n",
        "data = data.fillna(med)\n",
        "na = data.isna().sum()\n",
        "# Series[帶入根你的資料筆數一樣多True/False list]\n",
        "na[na > 0].sort_values(ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w7vN5dbewXo8"
      },
      "source": [
        "#錄製影片有詳細解說\n",
        "\n",
        "def name_convert(s):\n",
        "    s = s.split(\",\")[-1].split(\".\")[0]\n",
        "    s = s.strip()\n",
        "    return s\n",
        "counts = data[\"Name\"].apply(name_convert).value_counts()\n",
        "whitelist = counts[counts > 50].index\n",
        "def name_convert(s):\n",
        "    s = s.split(\",\")[-1].split(\".\")[0]\n",
        "    s = s.strip()\n",
        "    if s in whitelist:\n",
        "        return s\n",
        "    else:\n",
        "        return None\n",
        "data[\"Name\"] = data[\"Name\"].apply(name_convert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kv956eRZwXbk"
      },
      "source": [
        "#再次檢視資料中其它欄位是否有需要ETL\n",
        "#PClass屬大小類別123,Sex屬二值型，略過不必做\n",
        "#整理完之後為２５個欄位，如下：\n",
        "data = pd.get_dummies(data)\n",
        "data = pd.get_dummies(data, columns=[\"Pclass\"])\n",
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CMEWPakBzutj"
      },
      "source": [
        "#新增一個family欄位，故合併 SibSp+Parch（兄弟姐妹父母）\n",
        "#以上是預處理的欄位嘗試能提昇Model正確率,千萬不要任意的刪除資料中的欄位\n",
        "data[\"Family\"] = data[\"SibSp\"] + data[\"Parch\"]\n",
        "data\n",
        "#整理完之後為２６個欄位，如下："
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9QH1MWL77Yl"
      },
      "source": [
        "print(len(data))\n",
        "print(data.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "94L5Os1G0fg3"
      },
      "source": [
        "# .loc (根據列編號)(X) .iloc (根據第幾個)(O)\n",
        "# .iloc [第一列, 第二列, 第三列...]\n",
        "x_train = data.iloc[:train_df.shape[0]]\n",
        "y_train = train_df[\"Survived\"]\n",
        "x_predict = data.iloc[train_df.shape[0]:]\n",
        "# x_train"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "647bb5FN0fd7"
      },
      "source": [
        "data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZKliBzw80fXo"
      },
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "params = {\n",
        "    # 1. 5 2. [1, 2, 3] 3. range\n",
        "    # 20~99\n",
        "    \"n_estimators\":range(20, 100),\n",
        "    # 3~10\n",
        "    \"max_depth\":range(3, 11)\n",
        "}\n",
        "clf = RandomForestClassifier()\n",
        "cv = GridSearchCV(clf, params, cv=10, n_jobs=-1)\n",
        "cv.fit(x_train, y_train)\n",
        "print(cv.best_score_)\n",
        "print(cv.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BayhSWY10tkv"
      },
      "source": [
        "#can look Scikit-learn/selection.cross_val_score\n",
        "clf = RandomForestClassifier(n_estimators=25, max_depth=6)\n",
        "scores = cross_val_score(clf, x_train, y_train, cv=10, n_jobs=-1)\n",
        "print(\"10:\", scores)\n",
        "print(\"average:\", np.average(scores))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBajCrvFxIYx"
      },
      "source": [
        "# Use Tree look feauture_importances based on RandomForest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5CE59COCxK9y"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.tree import plot_tree\n",
        "print(len(clf.estimators_))\n",
        "plt.figure(figsize=(10, 10))\n",
        "plot_tree(clf.estimators_[2], \n",
        "          feature_names=data.columns, \n",
        "          class_names=[\"Dead\", \"Alived\"],\n",
        "          max_depth=2,\n",
        "          filled=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YnO08yKdxH29"
      },
      "source": [
        "pd.DataFrame({\n",
        "    \"Name\":data.columns,\n",
        "    \"Importance\":clf.feature_importances_\n",
        "}).sort_values(by=\"Importance\", ascending=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-VVxijV0fQk"
      },
      "source": [
        "#了解feature importances之後，預測分數後上傳至Kaggle\n",
        "clf = RandomForestClassifier(n_estimators=97,max_depth=8)\n",
        "clf.fit(x_train, y_train)\n",
        "pre = clf.predict(x_predict)\n",
        "df = pd.DataFrame({\n",
        "    \"PassengerId\":test_df[\"PassengerId\"],\n",
        "    \"Survived\":pre\n",
        "})\n",
        "df.to_csv(\"rflai.csv\",encoding=\"utf-8\",index=False)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIgfYu01LFgr"
      },
      "source": [
        "#嘗試使用KNN. (需要做Scaler), 將中心點定住"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1Jm9on0F0fFA"
      },
      "source": [
        "\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
        "scaler = MinMaxScaler()\n",
        "data_scale = scaler.fit_transform(data)\n",
        "#再一次將Data轉為DataFrame\n",
        "data_scale = pd.DataFrame(data_scale, columns=data.columns)\n",
        "# .loc (根據列編號)(X) .iloc (根據第幾個)(O)\n",
        "# .iloc [第一列, 第二列, 第三列...]\n",
        "x_train_scale = data_scale.iloc[:train_df.shape[0]]\n",
        "x_predict_scale = data_scale.iloc[train_df.shape[0]:]\n",
        "x_train_scale"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "69gCpS3gLY7D"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "params = {\n",
        "    \"n_neighbors\":range(3, 100)\n",
        "}\n",
        "clf = KNeighborsClassifier()\n",
        "cv = GridSearchCV(clf, params, cv=10, n_jobs=-1)\n",
        "cv.fit(x_train_scale, y_train)\n",
        "print(cv.best_score_)\n",
        "print(cv.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgJJe_ztL7lX"
      },
      "source": [
        "\n",
        "clf = KNeighborsClassifier(n_neighbors=11)\n",
        "clf.fit(x_train_scale, y_train)\n",
        "pre = clf.predict(x_predict_scale)\n",
        "df = pd.DataFrame({\n",
        "    \"PassengerId\":test_df[\"PassengerId\"],\n",
        "    \"Survived\":pre\n",
        "})\n",
        "df.to_csv(\"knn.csv\", encoding=\"utf-8\", index=False)\n",
        "df\n",
        "#In kaggle I submission scored 0.80143"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RY6PeThXupYE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pFZVcOpIupPI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sD8vURFYupA7"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j7oc44OmioHI"
      },
      "source": [
        "# 嘗試使用GradientBoostingClassifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0PGj8OzFMXAa"
      },
      "source": [
        "import pandas as pd\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "combined = pd.concat([train.drop('Survived',axis=1),test])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nTE42ViYk1AR"
      },
      "source": [
        "combined.info()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SNXG5p4Klvt7"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "train = pd.read_csv(\"train.csv\")\n",
        "test = pd.read_csv(\"test.csv\")\n",
        "train['Age'].fillna(train['Age'].median(),inplace=True) # Imputing Missing Age Values\n",
        "train['Embarked'].fillna(train['Embarked'].value_counts().index[0], inplace=True) # Imputing Missing Embarked Values\n",
        "d = {1:'1st',2:'2nd',3:'3rd'} #Creating a dictionary to convert Passenger Class from 1,2,3 to 1st,2nd,3rd.\n",
        "train['Pclass'] = train['Pclass'].map(d) #Mapping the column based on the dictionary\n",
        "train.drop(['PassengerId','Name','Ticket','Cabin'], 1, inplace=True) # Dropping Unnecessary Columns\n",
        "categorical_vars = train[['Pclass','Sex','Embarked']] # Getting Dummies of Categorical Variables\n",
        "dummies = pd.get_dummies(categorical_vars,drop_first=True)\n",
        "train = train.drop(['Pclass','Sex','Embarked'],axis=1) #Dropping the Original Categorical Variables to avoid duplicates\n",
        "train = pd.concat([train,dummies],axis=1) #Now, concat the new dummy variables\n",
        "train.head() #Check the clean version of the train data."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PyjzX9wGmEeN"
      },
      "source": [
        "# Splitting Features and Label\n",
        "y = train['Survived']\n",
        "X = train.drop(['Survived'],1)\n",
        "\n",
        "#Using Train Test Split from Sklearn to Split Our Train Dataset into Train and Testing Datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2LhdD58PmOuC"
      },
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "model = GradientBoostingClassifier(learning_rate=0.1,max_depth=3)\n",
        "model.fit(X_train, y_train)\n",
        "predictions = model.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BCDKvUHtmTbZ"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "print(confusion_matrix(y_test, predictions))\n",
        "print(classification_report(y_test,predictions))\n",
        "#I get about 80% accuracy"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmxhNx02mXwJ"
      },
      "source": [
        "test['Age'].fillna(test['Age'].median(),inplace=True) # Age\n",
        "test['Fare'].fillna(test['Fare'].median(),inplace=True) # Fare\n",
        "d = {1:'1st',2:'2nd',3:'3rd'} #Pclass\n",
        "test['Pclass'] = test['Pclass'].map(d)\n",
        "test['Embarked'].fillna(test['Embarked'].value_counts().index[0], inplace=True) # Embarked\n",
        "ids = test[['PassengerId']]# Passenger Ids\n",
        "test.drop(['PassengerId','Name','Ticket','Cabin'],1,inplace=True)# Drop Unnecessary Columns\n",
        "categorical_vars = test[['Pclass','Sex','Embarked']]# Get Dummies of Categorical Variables\n",
        "dummies = pd.get_dummies(categorical_vars,drop_first=True)\n",
        "test = test.drop(['Pclass','Sex','Embarked'],axis=1)#Drop the Original Categorical Variables\n",
        "test = pd.concat([test,dummies],axis=1)#Instead, concat the new dummy variables\n",
        "#test.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KdgL4iyrmrLj"
      },
      "source": [
        "preds = model.predict(test)\n",
        "results = ids.assign(Survived=preds)\n",
        "results.to_csv('titanic_submission.csv',index=False)\n",
        "#In kaggle I submission scored 0.73684"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}